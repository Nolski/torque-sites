#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#
# Perform some one-time fixes to the MacArthur Foundation 100&Change CSV file
#
# This is necessary because not everything can be addressed with the
# 'sanitize' script.  The 'sanitize' script isn't aware of CSV rows or
# columns; it just treats the whole CSV file as a normal text file.
# But for, e.g., fixing the YouTube links (see features.org for more),
# we really need a one-time transformation that knows what cell it's
# operating on.
#
# Copyright (C) 2017 Open Tech Strategies, LLC
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>.

##########################################################################
#                                                                        #
#   NOTE: This code is highly specific to the needs of the MacArthur     #
#   Foundation and is unlikely to be correct for your CSV.  It is        #
#   open source software, so please modify it to suit your needs.        #
#                                                                        #
##########################################################################

__doc__ = """\
Compose all of the MacArthur Foundation 2019 Proposal CSV files.

Usage:

  $ compose-csvs \\
       --proposals-csv=PROPOSALS_CSV \\
       --admin-review-csv=ADMIN_REVIEW_CSV \\
       --judge-evaluation-csv=JUDGE_EVALUATION_CSV \\
       --wisehead-evaluation-csv=WISEHEAD_CSV \\
       --correction-file=COORECTION_FILE

Command-line options:
  --proposals-csv FILE            FILE is a csv FILE representing the bulk
                                  of the proposal information

  --admin-review-csv FILE         FILE is a csv FILE representing which applications
                                  in PROPOSALS_CSV should be included

  --judge-evaluation-csv FILE     FILE is a csv FILE with a many to one relationshp
                                  between judges and the proposals they evaluated,
                                  with the extra data being their evaluation

  --pare N                        Reduce the nuber of tiems to 1/N rows, for quick
                                  testing and development

  --wisehead-evaluation-csv FILE  FILE is a csv FILE with a many to one relationship
                                  between wisehead judges and the proposals they
                                  evaluated, like the judge-evaluation-csv

  --correction-file FILE          FILE is a csv of corrections to the main data.  The header
                                  must match the header of the original proposals file, and any
                                  one of the columns must contain the review number.  Then
                                  the data from the correction file will override the
                                  source data for output.  There can be multiple correction
                                  files, and each one overwrites the previous.
"""

import csv
import getopt
import re
import io
import sys
import warnings
import string
import os
from bs4 import BeautifulSoup

def collapse_replace(string, old, new):
    "Return STRING, with OLD repeatedly replaced by NEW until no more OLD."
    while string.find(old) != -1: 
        string = string.replace(old, new)
    return string

def weaken_the_strong(html):
    """Strip any meaningless <strong>...</strong> tags from HTML.
    HTML is a Unicode string; the return value is either HTML or a new
    Unicode string based on HTML.

    If <strong> tags cover everything in HTML, remove the tags.  But if
    <strong> tags are used only sometimes, maybe they're meaningful, so
    leave them.  Basically, we want to make strength great again."""
    # If there's no strength here, move on.
    if not "<strong>" in html:
        return html
    
    # Remove the stuff inside strong tags
    soup = BeautifulSoup(html, "html.parser")
    while "<strong>" in str(soup):
        soup.strong.extract()

    # Check whether the non-bold stuff is more than just tags and
    # punctuation.  If not, all the important stuff was bold, so strip
    # bold and return.
    if re.sub(r"\W", "", soup.get_text()) == "":
        return re.sub("</?strong>", "", html)

    # OTOH, if the non-bold stuff contained letters or numbers, maybe
    # there's real content there, which means the html was a mix of
    # bold and non-bold text.  Better to leave it alone.
    return html

def form_well(html):
    """Return a well-formed version of HTML with dangling tags closed.
    Some of the input includes tables that cut off without closing
    tags; some entries leave <td> tags open too."""
    # Parse html and suppress warnings about urls in the text
    warnings.filterwarnings("ignore",
                            category=UserWarning, module='bs4')
    soup = BeautifulSoup(html, "html.parser")

    # Return well-formed html as a soup object
    return soup

# Used in converting "<foo>&nbsp;<bar>" and "</foo>&nbsp;<bar>"
# to "<foo> <bar>" and "</foo> <bar>" respectively.  (Those particular
# instances of "&nbsp;" in the data are not very convincing, and they
# create noise when we're looking for unnecessary escaping elsewhere.)
intertag_nbsp_re = re.compile('(?m)(</?[a-z]+>)&nbsp;(<[a-z]+>)')

# Matches Unicode 8226 (U+2022) at the beginning of a line,
# which is something that applicants do in a lot of fields.
bullets_re = re.compile("^â€¢", re.MULTILINE)

def fix_cell(cell):
    """Return a cleaned up version of the CELL that will work well
    with mediawiki, mainly through text subsitution."""
    # A straight-up HTML-unescaping might be the right thing
    # (i.e., cell = html_parser.unescape(cell) below)
    # in the long run, but for now, let's do the same limited
    # set of unescapings the original 'sanitize' script did:
    cell = cell.replace('&amp;', '&')
    cell = cell.replace('&lt;', '<')
    cell = cell.replace('&gt;', '>')
    # The rest should be recursively collapsing replacements:
    cell = collapse_replace(cell, '\t', ' ')
    cell = collapse_replace(cell, '&nbsp;&nbsp;', '&nbsp;')
    cell = collapse_replace(cell, '&nbsp; ', ' ')
    cell = collapse_replace(cell, ' &nbsp;', ' ')
    cell = collapse_replace(cell, '&nbsp;</', '</')
    cell = collapse_replace(cell, '\\"', '"')
    cell = re.sub(intertag_nbsp_re, '\\1 \\2', cell)

    soup = form_well(cell)
    cell = weaken_the_strong(str(soup))

    # The parsing for lists requires an asterisk at the start
    # of the line, but some entries use bullets. This regex
    # swaps the bullets for asterisks.
    cell = bullets_re.sub("*", cell)

    # We don't want to have extra new lines added at the beginning or end
    # because that could make the wiki formatting odd
    cell = cell.strip()

    return cell

def print_csv(header_row, rows, output):
    """Print the HEADER_ROW and ROWS to OUTPUT via csv"""

    csv_writer = csv.writer(output,
                            delimiter=',', quotechar='"', lineterminator="\n")

    csv_writer.writerow(header_row)
    for row in rows:
        csv_writer.writerow(row)

valid_traits = [ "IMPACTFUL", "EVIDENCE-BASED", "FEASIBLE", "DURABLE" ]

def process_evaluation_data(*csv_reader, app_col, score_rank_col, sum_of_scores_col, trait_col,
        score_normalized_col, comments_col):
    """Takes a CSV_READER representing evaluation data in a spreadsheet and
    turns that into a dict of dicts in the following form:

      EVALUATION_DATA[app_id] = {
        overall_score_rank_normalized: string,
        sum_of_scores_normalized: string,
        traits: array of TRAIT (below)
      }

      TRAIT = {
        name: string,
        score_normalized: string
        comments: concatenated string
      }

    The columns that data is looked up are required named integer arguments as follows:
      - APP_COL: column with application number
      - SCORE_RANK_COL: column with normalized score rank
      - SUM_OF_SCORES_COL: column with normalized scores
      - TRAIL_COL: column with the trait name
      - SCORE_NORMALIZED_COL: column with the normalized score
      - COMMENTS_COL: column with the comments

    The evaluation data coming in has many comments in their own row to one
    application.  There are N traits, and M judges per trait, with things like
    overall_score_rank_normalized being duplicated for all NxM rows.

    The name of the trait has to be one of:
      - IMPACTFUL
      - EVIDENCE-BASED
      - FEASIBLE
      - DURABLE

    The reason we don't dynamically load this is that for the purposes of
    the json export, as well as the spreadsheet headers, we need to know
    the names of the traits.

    The scores for the traits are added up based here rather than in the
    spreasheet.  Then, comments are concatenated for that trait.
    """

    evaluation_data = {}

    next(csv_reader[0])
    for row in csv_reader[0]:
        application_id = row[app_col]
        if not application_id in evaluation_data:
            evaluation_data[application_id] = {
                    "overall_score_rank_normalized": row[score_rank_col],
                    "sum_of_scores_normalized": row[sum_of_scores_col],
                    "traits": [{ "name": trait, "score_normalized": 0, "comments": "" } for trait in valid_traits],
                    }

        judge_datum = evaluation_data[application_id]

        found = False

        if row[trait_col] not in valid_traits:
            raise Exception("Trait is not a valid trait: " + row[trait_col])

        for trait in judge_datum["traits"]:
            if trait["name"] == row[trait_col]:
                trait["score_normalized"] += float(row[score_normalized_col])
                trait["comments"] += "\n\n* " + row[comments_col]

    return evaluation_data

def wiki_escape_page_title(s):
    import unidecode
    """Return a wiki-escaped version of STRING."""
    for c in ["#", "<", ">", "[", "]", "{", "|", "}",]:
        if s.find(c) != -1:
            s = s.replace(c, "-")
    while len(bytes(s, "UTF-8")) > 255:
        s = s[:-1]

    # Also convert non unicode because we do this with titles on the other side
    return unidecode.unidecode_expect_nonascii(s).strip()

def create_correction_data(correction_files):
    """Turns CORRECTION_FILES into a dictionary of dictionaries of the form:

    correction_data = {
      REVIEW_NUMBER_1 = {
        'CORRECTION_HEADER_1' = ...,
        'CORRECTION_HEADER_2' = ...,
        ...
      },
      REVIEW_NUMBER_1 = { ... },
      ...
    }"""

    correction_data = {}

    for correction_file in correction_files:
        correction_reader = csv.reader(open(correction_file, encoding='utf-8'),
                                       delimiter=',', quotechar='"')
        header = next(correction_reader)
        review_number_idx = header.index("Review Number")

        for correction_row in correction_reader:
            review_number = correction_row[review_number_idx]
            if review_number not in correction_data:
                correction_data[review_number] = {}
            for col_name, datum in zip(header, correction_row):
                correction_data[review_number][col_name] = datum

    return correction_data

def compose_csvs(proposals, admin_review, judge_eval, wisehead_eval, correction_files, pare=None):
    """Write a composed version of PROPOSALS (a CSV file), ADMIN_REVIEW
    (a CSV file), and JUDGE_EVAL (a CSV file) to standard out.

    PROPOSALS, ADMIN_REVIEW, and JUDGE_EVAL are all filenames.  They
    represent a full list of proposals, a list of accepted proposals,
    and evaluation on those proposals, respectively

    CORRECTION_FILES is a list of files that contain data to overwrite the
    source data with blessed corrections.  Each one overwrites the previous
    if there are overlapping corrections.

    If PARE is not None, it is a positive integer indicating that only
    1 of every PARE entries should be processed, and the others skipped."""
    try:
        proposals_reader = csv.reader(open(proposals, encoding='utf-8'),
                                      delimiter=',', quotechar='"')
        admin_review_reader = csv.reader(open(admin_review, encoding='utf-8'),
                                         delimiter=',', quotechar='"')
        judge_eval_reader = csv.reader(open(judge_eval, encoding='utf-8'),
                                       delimiter=',', quotechar='"')
        wisehead_eval_reader = csv.reader(open(wisehead_eval, encoding='utf-8'),
                                       delimiter=',', quotechar='"')
    except UnicodeDecodeError:
        sys.stderr.write("fix-csv expects utf-8-encoded unicode, not whatever is in this csv file.\n")
        sys.exit(-1)

    header_row = next(proposals_reader)
    for data_type in ["Judge", "Wise Head"]:
        header_row.append(data_type + " Overall Score Rank Normalized")
        header_row.append(data_type + " Sum of Scores Normalized")
        for trait in valid_traits:
            header_row.append(data_type + " " + trait)
            header_row.append(data_type + " " + trait + " Score Normalized")
            header_row.append(data_type + " " + trait + " Comments")
    header_row.append("Application Level")

    # We only accept applications if they're one of the rows in the
    # admin_review csv, which has an "Application #" column (3),
    # which matches a "Review #" column in the proposals (3)
    # and for which the "Status" column (7) is valid
    next(admin_review_reader) # Don't look at header
    acceptable_application_numbers = { row[3]: row[7] for row in admin_review_reader }

    judge_data = process_evaluation_data(judge_eval_reader,
            app_col=3, score_rank_col=9, sum_of_scores_col=11,
            trait_col=15, score_normalized_col=13, comments_col=14)
    wisehead_data = process_evaluation_data(wisehead_eval_reader,
            app_col=3, score_rank_col=11, sum_of_scores_col=13,
            trait_col=19, score_normalized_col=17, comments_col=18)

    correction_data = create_correction_data(correction_files)

    row_num = 0
    new_rows = []

    extra_cats = []
    for row in proposals_reader:
        if row[3] not in acceptable_application_numbers.keys():
            continue

        row_num += 1  # first row here is row 1 (the header row was row 0) 
        if pare is not None and row_num % pare != 0:
            continue

        if row[3] in wisehead_data:
            rank = int(wisehead_data[row[3]]["overall_score_rank_normalized"])
        else:
            # Marker value for invalid proposals that didn't get scored
            rank = 9999

        if row[3] in correction_data:
            for col_header, correction_value in correction_data[row[3]].items():
                col_for_correction = header_row.index(col_header)
                row[col_for_correction] = correction_value

        new_row = []
        extra_output_row = []
        num_fixed_cells = 0
        for cell in row:
            fixed_cell = fix_cell(cell)
            if fixed_cell != cell:
                num_fixed_cells += 1
            new_row.append(fixed_cell)
            extra_output_row.append(fixed_cell)

        for evaluation_data in [judge_data, wisehead_data]:
            if row[3] in evaluation_data:
                new_row.extend([
                    evaluation_data[row[3]]["overall_score_rank_normalized"],
                    evaluation_data[row[3]]["sum_of_scores_normalized"] ])
    
                for valid_trait in valid_traits:
                    for trait in evaluation_data[row[3]]["traits"]:
                        if trait["name"] == valid_trait:
                            new_row.append(trait["name"])
                            new_row.append("{0:.1f}".format(trait["score_normalized"]))
                            new_row.append(trait["comments"])

            else:
                new_row.extend([9999, ""])
    
                # for now, we know that there are 4 traits
                for _ in range(4):
                    new_row.extend(["", "", ""])

        new_row.append(acceptable_application_numbers[row[3]])

        print("Sanitized row %d (rank %d, %d cols, %d fixed)." % (row_num, rank, len(new_row), num_fixed_cells), file=sys.stderr)
        new_rows.append(new_row)

    new_rows.sort(key=lambda row: int(row[174])) # Sort by wisehead rank

    print_csv(header_row, new_rows, sys.stdout)

def main():
    """Compose the MacFound input and emit it as html-ized csv."""
    try:
        opts, args = getopt.getopt(sys.argv[1:], '',
                                   ["pare=",
                                    "proposals-csv=",
                                    "admin-review-csv=",
                                    "judge-evaluation-csv=",
                                    "wisehead-evaluation-csv=",
                                    "correction-file=",])
    except getopt.GetoptError as err:
        sys.stderr.write("ERROR: '%s'\n" % err)
        sys.exit(2)

    pare = None
    proposals_csv = None
    admin_review_csv = None
    judge_evaluation_csv = None
    wisehead_evaluation_csv = None
    correction_files = []
    for o, a in opts:
        if o == "--pare":
            pare = int(a)
        elif o == "--proposals-csv":
            proposals_csv = a
        elif o == "--admin-review-csv":
            admin_review_csv = a
        elif o == "--judge-evaluation-csv":
            judge_evaluation_csv = a
        elif o == "--wisehead-evaluation-csv":
            wisehead_evaluation_csv = a
        elif o == "--correction-file":
            correction_files.append(a)
        else:
            sys.stderr.write("ERROR: unrecognized option '%s'\n" % o)
            sys.exit(2)

    if (proposals_csv is None or
            admin_review_csv is None or
            judge_evaluation_csv is None or
            wisehead_evaluation_csv is None):
        sys.stderr.write(
            "ERROR: need --proposals-csv, --admin-review-csv, " +
            "--judge-evaluation-csv, and --wisehead_evaluation-csv options.\n\n")
        sys.stderr.write(__doc__)
        sys.exit(1)
    compose_csvs(proposals_csv, admin_review_csv, judge_evaluation_csv, wisehead_evaluation_csv, correction_files, pare)

if __name__ == '__main__':
    main()
